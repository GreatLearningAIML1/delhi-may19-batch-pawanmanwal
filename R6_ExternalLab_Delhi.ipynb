{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_ExternalLab_Delhi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYk8NG3yOIT9",
        "colab_type": "text"
      },
      "source": [
        "### A MNIST-like fashion product database\n",
        "\n",
        "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFO6PuxzOIT_",
        "colab_type": "text"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efNjNImfOIUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FceyHN_efEOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5567809c-f9c7-4ec8-cf3d-257c044f439e"
      },
      "source": [
        "tf.enable_eager_execution"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tensorflow.python.framework.ops.enable_eager_execution>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9C4aAIGOIUH",
        "colab_type": "code",
        "outputId": "29d3be44-c931-421f-c3bf-61f2bba55d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcoZBStrOIUQ",
        "colab_type": "text"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA1WsFSeOIUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnbx7TyQOIUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(trainX, trainY), (testX, testY) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbiHj5YPOIUc",
        "colab_type": "code",
        "outputId": "78ec0398-c66a-49cf-eb44-c0ae36a12ce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(testY[0:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDAYzkwyOIUj",
        "colab_type": "text"
      },
      "source": [
        "### Convert both training and testing labels into one-hot vectors.\n",
        "\n",
        "**Hint:** check **tf.keras.utils.to_categorical()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BcLcEgKReQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67561406-d4bd-4a53-9f9b-e81612436cb4"
      },
      "source": [
        "trainX[0:1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           1,   0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,\n",
              "           0,   1,   1,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           3,   0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,\n",
              "           4,   0,   0,   3],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           6,   0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,\n",
              "           0,  12,  10,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,\n",
              "          77, 130,  72,  15],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "           0,  69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146,\n",
              "         141,  88, 172,  66],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,\n",
              "           0, 200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127,\n",
              "         123, 196, 229,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0, 183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221,\n",
              "         223, 245, 173,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0, 193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223,\n",
              "         220, 243, 202,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,\n",
              "          12, 219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212,\n",
              "         226, 197, 209,  52],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,\n",
              "          99, 244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220,\n",
              "         245, 119, 167,  56],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,\n",
              "          55, 236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217,\n",
              "         217, 209,  92,   0],\n",
              "        [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0,\n",
              "         237, 226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215,\n",
              "         218, 255,  77,   0],\n",
              "        [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204,\n",
              "         228, 207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215,\n",
              "         224, 244, 159,   0],\n",
              "        [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222,\n",
              "         217, 226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248,\n",
              "         233, 238, 215,   0],\n",
              "        [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209,\n",
              "         200, 159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211,\n",
              "         220, 232, 246,   0],\n",
              "        [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220,\n",
              "         240,  80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209,\n",
              "         222, 228, 225,   0],\n",
              "        [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215,\n",
              "         217, 241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223,\n",
              "         223, 224, 229,  29],\n",
              "        [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206,\n",
              "         198, 213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222,\n",
              "         220, 221, 230,  67],\n",
              "        [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214,\n",
              "         219, 221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172,\n",
              "         181, 205, 206, 115],\n",
              "        [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207,\n",
              "         211, 210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156,\n",
              "         167, 177, 210,  92],\n",
              "        [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188,\n",
              "         189, 188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194,\n",
              "         192, 216, 170,   0],\n",
              "        [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243,\n",
              "         244, 221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,\n",
              "          99,  58,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,\n",
              "          35,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBlfYlANOIUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(testY, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RHV3b9mzOIUq",
        "colab_type": "code",
        "outputId": "79082007-234b-4cb3-a750-c64ad1816bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "print(trainY.shape)\n",
        "print('First 5 examples now are: ', trainY[0:5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n",
            "First 5 examples now are:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwhQ8e7VOIUw",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the data\n",
        "\n",
        "Plot first 10 images in the triaining set and their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyySxQr4UyH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "04a5ba9c-4656-4f02-e1aa-bd36d528dacf"
      },
      "source": [
        "#Lets print the image as well\n",
        "import matplotlib.pyplot as plt\n",
        "fig=plt.figure(figsize=(10, 10))\n",
        "for i in range(1, 11,1):\n",
        "  fig.add_subplot(1, 10, i)\n",
        "  plt.imshow(trainX[i-1],cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlIAAABRCAYAAAAZ1Ej0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4TbJGeSOIU4",
        "colab_type": "text"
      },
      "source": [
        "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGz3GqaZdpFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "c4787773-8591-46c4-8b09-4f52623770d1"
      },
      "source": [
        "#Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCzdAAYzdhIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPPOTpuIej6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Comile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hQpLv3aOIU_",
        "colab_type": "text"
      },
      "source": [
        "### Execute the model using model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O59C_-IgOIVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "885cd5a4-842d-4861-b001-2cda2124ff0e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwTR4slMeJox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "de7866d0-06e2-4284-b692-735b74a82661"
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=10,\n",
        "          batch_size=trainX.shape[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 0s 5us/sample - loss: 243.8430 - acc: 0.0538 - val_loss: 4236.4482 - val_acc: 0.2033\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 4227.7217 - acc: 0.2084 - val_loss: 9876.5684 - val_acc: 0.1811\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 9811.1318 - acc: 0.1815 - val_loss: 14353.8301 - val_acc: 0.2391\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 14240.5566 - acc: 0.2439 - val_loss: 18444.3574 - val_acc: 0.2859\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 18374.1191 - acc: 0.2892 - val_loss: 14153.8594 - val_acc: 0.2687\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 14100.5762 - acc: 0.2694 - val_loss: 13631.3184 - val_acc: 0.2873\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 13565.2227 - acc: 0.2880 - val_loss: 5275.3633 - val_acc: 0.2585\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 5243.5703 - acc: 0.2603 - val_loss: 7847.7104 - val_acc: 0.3744\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 7768.3774 - acc: 0.3777 - val_loss: 7252.7607 - val_acc: 0.4554\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 7207.4082 - acc: 0.4588 - val_loss: 7094.2798 - val_acc: 0.5175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f206a649f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdzDtGwDOIVF",
        "colab_type": "text"
      },
      "source": [
        "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kndfpdidOIVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "#Comile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPQfzoSRoTdv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "6e86a415-4a67-4474-b7c0-294f07967a45"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 10)                40        \n",
            "=================================================================\n",
            "Total params: 7,890\n",
            "Trainable params: 7,870\n",
            "Non-trainable params: 20\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwk3T5LJOIVN",
        "colab_type": "text"
      },
      "source": [
        "### Execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNLR8tcBOIVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "e98d2826-0975-4776-bb7c-dc624d25058e"
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=10,\n",
        "          batch_size=101)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 26us/sample - loss: 10.7585 - acc: 0.5420 - val_loss: 11.8194 - val_acc: 0.5461\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 10.4463 - acc: 0.5122 - val_loss: 10.8523 - val_acc: 0.5129\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 10.5988 - acc: 0.5167 - val_loss: 10.8750 - val_acc: 0.5146\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 10.5141 - acc: 0.5179 - val_loss: 10.8604 - val_acc: 0.5132\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 10.4525 - acc: 0.5177 - val_loss: 10.7414 - val_acc: 0.5144\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 9.1059 - acc: 0.4001 - val_loss: 9.2502 - val_acc: 0.3137\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 7.5605 - acc: 0.3890 - val_loss: 8.5813 - val_acc: 0.4057\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 7.5788 - acc: 0.4012 - val_loss: 12.1150 - val_acc: 0.4453\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 9.6981 - acc: 0.4520 - val_loss: 12.1229 - val_acc: 0.4455\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 10.0011 - acc: 0.4817 - val_loss: 11.7582 - val_acc: 0.5052\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2064f9ffd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py-KwkmjOIVU",
        "colab_type": "text"
      },
      "source": [
        "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLXUE9jWOIVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras import optimizers\n",
        "sgd = tf.keras.optimizers.SGD(lr=0.0001)\n",
        "\n",
        "#Comile the model\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJUqA5T4OIVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "8fb83f19-bc87-4187-96a4-fab42f43e650"
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=10,\n",
        "          batch_size=100)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 26us/sample - loss: 10.9777 - acc: 0.5096 - val_loss: 11.7582 - val_acc: 0.5052\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 10.8813 - acc: 0.5096 - val_loss: 11.7582 - val_acc: 0.5052\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 11.0038 - acc: 0.5096 - val_loss: 11.7582 - val_acc: 0.5052\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 10.9183 - acc: 0.5096 - val_loss: 11.7582 - val_acc: 0.5052\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 11.0152 - acc: 0.5096 - val_loss: 11.7582 - val_acc: 0.5052\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 11.0431 - acc: 0.5096 - val_loss: 11.7582 - val_acc: 0.5052\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 11.0194 - acc: 0.5096 - val_loss: 11.7582 - val_acc: 0.5052\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 10.8552 - acc: 0.5096 - val_loss: 11.7582 - val_acc: 0.5052\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 10.9573 - acc: 0.5096 - val_loss: 11.7582 - val_acc: 0.5052\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 10.8897 - acc: 0.5096 - val_loss: 11.7582 - val_acc: 0.5051\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2064e98a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9CSqKvpOIVk",
        "colab_type": "text"
      },
      "source": [
        "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGAad54JOIVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize Sequential model\n",
        "model2 = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model2.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ7oIymROIVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add 1st hidden layer\n",
        "model2.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "\n",
        "#Add 2nd hidden layer\n",
        "model2.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "\n",
        "#Add OUTPUT layer\n",
        "model2.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-O-fFxnOIVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras import optimizers\n",
        "sgd = tf.keras.optimizers.SGD(lr=0.03)\n",
        "\n",
        "#Comile the model\n",
        "model2.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr2YsZV0OIV0",
        "colab_type": "text"
      },
      "source": [
        "## Review model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4ojW6-oOIV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "a8d64ae7-fe89-4466-91a9-4f83a915f908"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_1 (Reshape)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 89,610\n",
            "Trainable params: 89,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfFGmbZLOIV5",
        "colab_type": "text"
      },
      "source": [
        "### Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIkbMEN5OIV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0faa64e8-6b87-4f93-8954-0e9bb5775c58"
      },
      "source": [
        "model2.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=100)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 1.4244 - acc: 0.6310 - val_loss: 0.9896 - val_acc: 0.7026\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.8201 - acc: 0.7405 - val_loss: 0.7396 - val_acc: 0.7460\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.6861 - acc: 0.7695 - val_loss: 0.6773 - val_acc: 0.7650\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6403 - acc: 0.7785 - val_loss: 0.6767 - val_acc: 0.7683\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6194 - acc: 0.7872 - val_loss: 0.6211 - val_acc: 0.7859\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6048 - acc: 0.7941 - val_loss: 0.6626 - val_acc: 0.7667\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6055 - acc: 0.7901 - val_loss: 0.6250 - val_acc: 0.7837\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6236 - acc: 0.7838 - val_loss: 0.6363 - val_acc: 0.7742\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6160 - acc: 0.7841 - val_loss: 0.6464 - val_acc: 0.7634\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6227 - acc: 0.7803 - val_loss: 0.6877 - val_acc: 0.7500\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6389 - acc: 0.7836 - val_loss: 0.7026 - val_acc: 0.7464\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6227 - acc: 0.7829 - val_loss: 0.6527 - val_acc: 0.7751\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6532 - acc: 0.7718 - val_loss: 0.6332 - val_acc: 0.7745\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6404 - acc: 0.7754 - val_loss: 0.6742 - val_acc: 0.7557\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6403 - acc: 0.7718 - val_loss: 0.6843 - val_acc: 0.7435\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6594 - acc: 0.7664 - val_loss: 0.6593 - val_acc: 0.7670\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6450 - acc: 0.7704 - val_loss: 0.6727 - val_acc: 0.7602\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6496 - acc: 0.7634 - val_loss: 0.6586 - val_acc: 0.7645\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6458 - acc: 0.7693 - val_loss: 0.7059 - val_acc: 0.7357\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6487 - acc: 0.7718 - val_loss: 0.6593 - val_acc: 0.7661\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6382 - acc: 0.7751 - val_loss: 0.6716 - val_acc: 0.7649\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6311 - acc: 0.7716 - val_loss: 0.6582 - val_acc: 0.7591\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6463 - acc: 0.7668 - val_loss: 0.6324 - val_acc: 0.7755\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6441 - acc: 0.7600 - val_loss: 0.7039 - val_acc: 0.7341\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6412 - acc: 0.7674 - val_loss: 0.6494 - val_acc: 0.7670\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6225 - acc: 0.7757 - val_loss: 0.6397 - val_acc: 0.7755\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6292 - acc: 0.7730 - val_loss: 0.6440 - val_acc: 0.7647\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6391 - acc: 0.7681 - val_loss: 0.6385 - val_acc: 0.7662\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6346 - acc: 0.7700 - val_loss: 0.6602 - val_acc: 0.7497\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6404 - acc: 0.7604 - val_loss: 0.6615 - val_acc: 0.7530\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6459 - acc: 0.7632 - val_loss: 0.6899 - val_acc: 0.7553\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6433 - acc: 0.7594 - val_loss: 0.6617 - val_acc: 0.7391\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6367 - acc: 0.7652 - val_loss: 0.6408 - val_acc: 0.7697\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6240 - acc: 0.7705 - val_loss: 0.6494 - val_acc: 0.7466\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6185 - acc: 0.7767 - val_loss: 0.6450 - val_acc: 0.7810\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6264 - acc: 0.7683 - val_loss: 0.6605 - val_acc: 0.7535\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6273 - acc: 0.7680 - val_loss: 0.6520 - val_acc: 0.7631\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.6228 - acc: 0.7732 - val_loss: 0.6518 - val_acc: 0.7581\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6121 - acc: 0.7750 - val_loss: 0.6537 - val_acc: 0.7657\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.6278 - acc: 0.7677 - val_loss: 0.6700 - val_acc: 0.7600\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6215 - acc: 0.7712 - val_loss: 0.6185 - val_acc: 0.7681\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6263 - acc: 0.7633 - val_loss: 0.6349 - val_acc: 0.7689\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6089 - acc: 0.7822 - val_loss: 0.6258 - val_acc: 0.7801\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 2s 35us/sample - loss: 0.6296 - acc: 0.7685 - val_loss: 0.6650 - val_acc: 0.7654\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 2s 36us/sample - loss: 0.6255 - acc: 0.7798 - val_loss: 0.6496 - val_acc: 0.7715\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 2s 35us/sample - loss: 0.6176 - acc: 0.7776 - val_loss: 0.6326 - val_acc: 0.7674\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6030 - acc: 0.7734 - val_loss: 0.6088 - val_acc: 0.7799\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6256 - acc: 0.7723 - val_loss: 0.6153 - val_acc: 0.7936\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6192 - acc: 0.7693 - val_loss: 0.6818 - val_acc: 0.7562\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6425 - acc: 0.7703 - val_loss: 0.6627 - val_acc: 0.7640\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6086 - acc: 0.7760 - val_loss: 0.6499 - val_acc: 0.7627\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.5906 - acc: 0.7819 - val_loss: 0.6414 - val_acc: 0.7571\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6029 - acc: 0.7756 - val_loss: 0.6580 - val_acc: 0.7663\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6112 - acc: 0.7749 - val_loss: 0.6564 - val_acc: 0.7646\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6127 - acc: 0.7762 - val_loss: 0.6577 - val_acc: 0.7665\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6096 - acc: 0.7758 - val_loss: 0.6564 - val_acc: 0.7602\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6386 - acc: 0.7660 - val_loss: 0.6231 - val_acc: 0.7613\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6462 - acc: 0.7587 - val_loss: 0.6478 - val_acc: 0.7612\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6245 - acc: 0.7695 - val_loss: 0.6685 - val_acc: 0.7554\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6220 - acc: 0.7737 - val_loss: 0.6324 - val_acc: 0.7820\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6165 - acc: 0.7768 - val_loss: 0.6241 - val_acc: 0.7777\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6120 - acc: 0.7782 - val_loss: 0.6633 - val_acc: 0.7584\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6126 - acc: 0.7792 - val_loss: 0.6566 - val_acc: 0.7569\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6050 - acc: 0.7768 - val_loss: 0.6509 - val_acc: 0.7651\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6011 - acc: 0.7793 - val_loss: 0.6386 - val_acc: 0.7715\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.6233 - acc: 0.7699 - val_loss: 0.6621 - val_acc: 0.7548\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6061 - acc: 0.7802 - val_loss: 0.6344 - val_acc: 0.7815\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6021 - acc: 0.7792 - val_loss: 0.6251 - val_acc: 0.7666\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6041 - acc: 0.7708 - val_loss: 0.6422 - val_acc: 0.7674\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6005 - acc: 0.7842 - val_loss: 0.6394 - val_acc: 0.7808\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6098 - acc: 0.7798 - val_loss: 0.6531 - val_acc: 0.7579\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6147 - acc: 0.7651 - val_loss: 0.6256 - val_acc: 0.7801\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6229 - acc: 0.7685 - val_loss: 0.6449 - val_acc: 0.7749\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5932 - acc: 0.7858 - val_loss: 0.6267 - val_acc: 0.7489\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6078 - acc: 0.7787 - val_loss: 0.6264 - val_acc: 0.7844\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5990 - acc: 0.7808 - val_loss: 0.6368 - val_acc: 0.7679\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6217 - acc: 0.7710 - val_loss: 0.6497 - val_acc: 0.7650\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6379 - acc: 0.7653 - val_loss: 0.6487 - val_acc: 0.7618\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6522 - acc: 0.7568 - val_loss: 0.6562 - val_acc: 0.7641\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6358 - acc: 0.7623 - val_loss: 0.6593 - val_acc: 0.7536\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6078 - acc: 0.7786 - val_loss: 0.6485 - val_acc: 0.7675\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6409 - acc: 0.7642 - val_loss: 0.7074 - val_acc: 0.7413\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6346 - acc: 0.7656 - val_loss: 0.6395 - val_acc: 0.7602\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6209 - acc: 0.7753 - val_loss: 0.6561 - val_acc: 0.7780\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6022 - acc: 0.7862 - val_loss: 0.6177 - val_acc: 0.7852\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6254 - acc: 0.7773 - val_loss: 0.6767 - val_acc: 0.7525\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6168 - acc: 0.7750 - val_loss: 0.6531 - val_acc: 0.7641\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6209 - acc: 0.7755 - val_loss: 0.6426 - val_acc: 0.7658\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6199 - acc: 0.7769 - val_loss: 0.6919 - val_acc: 0.7508\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6423 - acc: 0.7605 - val_loss: 0.6598 - val_acc: 0.7550\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6237 - acc: 0.7715 - val_loss: 0.6302 - val_acc: 0.7583\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6094 - acc: 0.7762 - val_loss: 0.6550 - val_acc: 0.7493\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6190 - acc: 0.7717 - val_loss: 0.6402 - val_acc: 0.7645\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6121 - acc: 0.7758 - val_loss: 0.6389 - val_acc: 0.7464\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.5977 - acc: 0.7834 - val_loss: 0.6167 - val_acc: 0.7752\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6050 - acc: 0.7779 - val_loss: 0.6513 - val_acc: 0.7594\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6172 - acc: 0.7759 - val_loss: 0.6745 - val_acc: 0.7563\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.6112 - acc: 0.7763 - val_loss: 0.6398 - val_acc: 0.7630\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6078 - acc: 0.7786 - val_loss: 0.6635 - val_acc: 0.7622\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.6021 - acc: 0.7809 - val_loss: 0.6430 - val_acc: 0.7535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2064d8cf60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAufkJ0CpbrQ",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy is ranging from 75% to 79%. Please suggest what further more can be done to get this accuracy to above 95%**"
      ]
    }
  ]
}